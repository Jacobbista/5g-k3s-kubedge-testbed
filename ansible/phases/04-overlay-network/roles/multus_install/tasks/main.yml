---
# MULTUS + WHEREABOUTS SETUP
# Deploy Multus as a meta-plugin (NOT primary CNI)
# Flannel (worker) and edge-cni (edge) remain the primary CNIs
# Multus is invoked only when pods have k8s.v1.cni.cncf.io/networks annotations

- name: Ensure namespaces for NADs
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata: { name: "{{ item }}" }
    kubeconfig: "/home/vagrant/kubeconfig"
  loop: ["5g", "mec"]
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Create Multus RBAC
- name: Create Multus ServiceAccount
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: multus
        namespace: kube-system
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Create Multus ClusterRole
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: multus
      rules:
        - apiGroups: ["k8s.cni.cncf.io"]
          resources:
            - "*"
          verbs:
            - "*"
        - apiGroups: ["whereabouts.cni.cncf.io"]
          resources:
            - ippools
            - overlappingrangeipreservations
          verbs:
            - get
            - list
            - watch
            - create
            - update
            - patch
            - delete
        - apiGroups:
            - ""
          resources:
            - pods
            - pods/status
            - namespaces
          verbs:
            - get
            - list
            - watch
            - update
        - apiGroups:
            - ""
            - events.k8s.io
          resources:
            - events
          verbs:
            - create
            - patch
            - update
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Create Multus ClusterRoleBinding
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      metadata:
        name: multus
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: ClusterRole
        name: multus
      subjects:
        - kind: ServiceAccount
          name: multus
          namespace: kube-system
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Multus NetworkAttachmentDefinition CRD
- name: Apply Multus CRD
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      metadata:
        name: network-attachment-definitions.k8s.cni.cncf.io
      spec:
        group: k8s.cni.cncf.io
        scope: Namespaced
        names:
          plural: network-attachment-definitions
          singular: network-attachment-definition
          kind: NetworkAttachmentDefinition
          shortNames:
            - net-attach-def
        versions:
          - name: v1
            served: true
            storage: true
            schema:
              openAPIV3Schema:
                type: object
                properties:
                  spec:
                    type: object
                    properties:
                      config:
                        type: string
    kubeconfig: "/home/vagrant/kubeconfig"
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Whereabouts CRDs
- name: Download Whereabouts CRDs
  get_url:
    url: "{{ item }}"
    dest: "/tmp/{{ item | basename }}"
    mode: "0644"
  loop: "{{ whereabouts_crds }}"
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Apply Whereabouts CRDs
  kubernetes.core.k8s:
    state: present
    src: "/tmp/{{ item | basename }}"
    kubeconfig: "/home/vagrant/kubeconfig"
  loop: "{{ whereabouts_crds }}"
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Whereabouts config on worker
- name: Ensure whereabouts.d exists on worker
  file:
    path: "{{ cni_conf_dir_worker }}/whereabouts.d"
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Read admin kubeconfig
  slurp:
    src: /home/vagrant/kubeconfig
  register: admin_kubeconfig
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Write Whereabouts kubeconfig (worker)
  copy:
    content: "{{ admin_kubeconfig.content | b64decode | regex_replace('https://127\\.0\\.0\\.1:6443', 'https://' + hostvars[groups['masters'][0]]['ansible_host'] + ':6443') }}"
    dest: "{{ cni_conf_dir_worker }}/{{ whereabouts_kubeconfig_relpath }}"
    mode: "0644"
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Write whereabouts.conf on worker
  copy:
    dest: "{{ cni_conf_dir_worker }}/{{ whereabouts_conf_relpath }}"
    mode: "0644"
    content: |
      {"datastore":"kubernetes","kubernetes":{"kubeconfig":"{{ cni_conf_dir_worker }}/{{ whereabouts_kubeconfig_relpath }}"}}
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Ensure /etc/cni/net.d exists on worker (for compatibility symlink)
  file:
    path: "/etc/cni/net.d"
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Create symlink for Whereabouts config on worker (k3s compatibility)
  file:
    src: "{{ cni_conf_dir_worker }}/whereabouts.d"
    dest: "/etc/cni/net.d/whereabouts.d"
    state: link
    force: yes
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

# Whereabouts config on edge
- name: Ensure whereabouts.d exists on edge
  file:
    path: "{{ cni_conf_dir_edge }}/whereabouts.d"
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

# Create primary CNI config for edge (MUST exist before EdgeCore starts)
- name: Create primary CNI config for edge
  copy:
    dest: "{{ cni_conf_dir_edge }}/10-edge.conflist"
    mode: "0644"
    content: |
      {
        "cniVersion": "1.0.0",
        "name": "edge-cni",
        "plugins": [
          {
            "type": "bridge",
            "bridge": "cni0",
            "isGateway": true,
            "isDefaultGateway": false,
            "ipMasq": true,
            "hairpinMode": true,
            "ipam": {
              "type": "host-local",
              "subnet": "10.244.0.0/24"
            }
          },
          {
            "type": "portmap",
            "capabilities": {
              "portMappings": true
            }
          }
        ]
      }
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

- name: Write Whereabouts kubeconfig (edge)
  copy:
    content: "{{ admin_kubeconfig.content | b64decode | regex_replace('https://127\\.0\\.0\\.1:6443', 'https://' + hostvars[groups['masters'][0]]['ansible_host'] + ':6443') }}"
    dest: "{{ cni_conf_dir_edge }}/{{ whereabouts_kubeconfig_relpath }}"
    mode: "0644"
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

- name: Write whereabouts.conf on edge
  copy:
    dest: "{{ cni_conf_dir_edge }}/{{ whereabouts_conf_relpath }}"
    mode: "0644"
    content: |
      {"datastore":"kubernetes","kubernetes":{"kubeconfig":"{{ cni_conf_dir_edge }}/{{ whereabouts_kubeconfig_relpath }}"}}
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

# ==============================================================
# MULTUS TOKEN AND KUBECONFIG SETUP
# ==============================================================

- name: Ensure Multus SA token secret
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: multus-sa-token
        namespace: kube-system
        annotations:
          kubernetes.io/service-account.name: multus
      type: kubernetes.io/service-account-token
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Wait for Multus SA token to be populated
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Secret
    name: multus-sa-token
    namespace: kube-system
    kubeconfig: "/home/vagrant/kubeconfig"
  register: multus_sa_secret
  until: multus_sa_secret.resources | length > 0 and (multus_sa_secret.resources[0].data.token | default('')) | length > 0
  retries: 20
  delay: 2
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# ==============================================================
# SELF-CONTAINED MULTUS KUBECONFIG GENERATION
# These tasks are independent and work even with --start-at-task
# ==============================================================

# Ensure kubeconfig directories on worker and edge
- name: Ensure multus.d exists on worker
  file:
    path: "{{ cni_conf_dir_worker }}/multus.d"
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Ensure multus.d exists on edge
  file:
    path: "{{ cni_conf_dir_edge }}/multus.d"
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

# Re-read admin kubeconfig (self-contained, works with --start-at-task)
- name: Read admin kubeconfig for Multus kubeconfig generation
  slurp:
    src: /home/vagrant/kubeconfig
  register: _multus_admin_kubeconfig
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Re-fetch Multus SA token (self-contained, works with --start-at-task)
- name: Get Multus SA token for kubeconfig generation
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Secret
    name: multus-sa-token
    namespace: kube-system
    kubeconfig: "/home/vagrant/kubeconfig"
  register: _multus_token_secret
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Derive kubeconfig fields (self-contained, no dependency on hostvars from other tasks)
- name: Derive Multus kubeconfig fields
  vars:
    _admin_obj: "{{ (_multus_admin_kubeconfig.content | b64decode | from_yaml) }}"
  set_fact:
    _multus_k8s_server: "https://{{ hostvars[groups['masters'][0]].ansible_host }}:6443"
    _multus_k8s_ca_b64: "{{ _admin_obj.clusters[0].cluster['certificate-authority-data'] }}"
    _multus_token: "{{ _multus_token_secret.resources[0].data.token | b64decode }}"
  run_once: true

# Validate inputs before writing kubeconfig
- name: Assert Multus kubeconfig inputs are valid
  assert:
    that:
      - _multus_k8s_server is match('^https://[0-9.]+:6443$')
      - _multus_k8s_ca_b64 | length > 0
      - _multus_token | length > 0
    fail_msg: "Multus kubeconfig inputs missing/invalid. Server={{ _multus_k8s_server }}, CA_len={{ _multus_k8s_ca_b64 | length }}, Token_len={{ _multus_token | length }}"
  run_once: true

# Create kubeconfig on worker with inline variables (no hostvars dependency)
- name: Create Multus kubeconfig on worker
  copy:
    dest: "{{ cni_conf_dir_worker }}/multus.d/multus.kubeconfig"
    mode: "0644"
    backup: true
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority-data: {{ _multus_k8s_ca_b64 }}
          server: {{ _multus_k8s_server }}
      users:
      - name: multus
        user:
          token: {{ _multus_token }}
      contexts:
      - name: multus-context
        context:
          cluster: local
          user: multus
      current-context: multus-context
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true
  no_log: true

# Create kubeconfig on edge with inline variables (no hostvars dependency)
# Path is /var/lib/multus/ to keep it outside CNI config dir and prevent
# thin_entrypoint from overwriting it (KubeEdge workaround)
- name: Ensure Multus kubeconfig directory on edge
  file:
    path: /var/lib/multus
    state: directory
    mode: "0755"
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

- name: Create Multus kubeconfig on edge
  copy:
    dest: /var/lib/multus/multus.kubeconfig
    mode: "0644"
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority-data: {{ _multus_k8s_ca_b64 }}
          server: {{ _multus_k8s_server }}
      users:
      - name: multus
        user:
          token: {{ _multus_token }}
      contexts:
      - name: multus-context
        context:
          cluster: local
          user: multus
      current-context: multus-context
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true
  no_log: true

# Sanity check: verify server line is correct on edge
- name: Sanity check kubeconfig server on edge
  shell: "grep 'server:' /var/lib/multus/multus.kubeconfig | awk '{print $2}'"
  register: _edge_server_check
  changed_when: false
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

- name: Ensure kubeconfig server is correct on edge
  assert:
    that: "_edge_server_check.stdout == _multus_k8s_server"
    fail_msg: "Edge kubeconfig server={{ _edge_server_check.stdout }} != expected={{ _multus_k8s_server }}"
  run_once: true

# ==============================================================
# STATIC MULTUS CONFIG FOR EDGE (KubeEdge workaround)
# See: docs/known-issues/kubeedge-multus-env-injection.md
# ==============================================================

- name: Create static Multus conflist on edge (KubeEdge workaround)
  template:
    src: 00-multus-edge.conflist.j2
    dest: "{{ cni_conf_dir_edge }}/00-multus.conflist"
    mode: "0644"
  delegate_to: "{{ groups['edges'][0] }}"
  run_once: true

# ==============================================================
# DEPLOY MULTUS AS META-PLUGIN (NO PRIMARY CNI CONFLIST)
# Multus will install binaries but NOT override the primary CNI
# ==============================================================

- name: Deploy Multus DaemonSet for worker (meta-plugin mode)
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: multus-worker
        namespace: kube-system
        labels:
          tier: node
          app: multus
          target: worker
      spec:
        selector:
          matchLabels:
            name: multus-worker
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              tier: node
              app: multus
              name: multus-worker
          spec:
            hostNetwork: true
            nodeSelector:
              kubernetes.io/hostname: worker
            tolerations:
              - operator: Exists
                effect: NoSchedule
              - operator: Exists
                effect: NoExecute
            serviceAccountName: multus
            containers:
              - name: kube-multus
                image: ghcr.io/k8snetworkplumbingwg/multus-cni:v{{ multus_version }}
                command: ["/thin_entrypoint"]
                args:
                  - "--multus-conf-file=auto"
                  - "--multus-log-level=verbose"
                  - "--multus-kubeconfig-file-host={{ cni_conf_dir_worker }}/multus.d/multus.kubeconfig"
                  - "--cni-version=1.0.0"
                  - "--multus-log-to-stderr=true"
                  - "--multus-log-file=/var/log/multus.log"
                  - "--skip-multus-binary-copy=false"
                resources:
                  requests:
                    cpu: "100m"
                    memory: "50Mi"
                  limits:
                    cpu: "100m"
                    memory: "50Mi"
                securityContext:
                  privileged: true
                  capabilities:
                    add: ["SYS_ADMIN"]
                volumeMounts:
                  - name: cni
                    mountPath: /host/etc/cni/net.d
                  - name: cnibin
                    mountPath: /host/opt/cni/bin
                env:
                  - name: KUBERNETES_NODE_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: spec.nodeName
                  - name: KUBERNETES_SERVICE_HOST
                    value: "{{ hostvars[groups['masters'][0]]['ansible_host'] }}"
                  - name: KUBERNETES_SERVICE_PORT
                    value: "6443"
            volumes:
              - name: cni
                hostPath:
                  path: "{{ cni_conf_dir_worker }}"
              - name: cnibin
                hostPath:
                  path: "{{ cni_bin_dir_worker }}"
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Edge DaemonSet uses STATIC config (not auto) to work around KubeEdge env var issue
# KubeEdge injects empty KUBERNETES_SERVICE_HOST/PORT which breaks kubeconfig generation
# See: docs/known-issues/kubeedge-multus-env-injection.md
- name: Deploy Multus DaemonSet for edge (static config mode - KubeEdge workaround)
  kubernetes.core.k8s:
    state: present
    kubeconfig: "/home/vagrant/kubeconfig"
    definition:
      apiVersion: apps/v1
      kind: DaemonSet
      metadata:
        name: multus-edge
        namespace: kube-system
        labels:
          tier: node
          app: multus
          target: edge
      spec:
        selector:
          matchLabels:
            name: multus-edge
        updateStrategy:
          type: RollingUpdate
        template:
          metadata:
            labels:
              tier: node
              app: multus
              name: multus-edge
          spec:
            hostNetwork: true
            nodeSelector:
              kubernetes.io/hostname: edge
            tolerations:
              - operator: Exists
                effect: NoSchedule
              - operator: Exists
                effect: NoExecute
            serviceAccountName: multus
            containers:
              - name: kube-multus
                image: ghcr.io/k8snetworkplumbingwg/multus-cni:v{{ multus_version }}
                command: ["/thin_entrypoint"]
                args:
                  # Use static conflist instead of auto - this SKIPS kubeconfig generation
                  # (--multus-kubeconfig-file-host is only used with auto mode)
                  - "--multus-conf-file=/host/etc/cni/net.d/00-multus.conflist"
                  - "--multus-log-level=verbose"
                  - "--multus-log-to-stderr=true"
                  - "--multus-log-file=/var/log/multus.log"
                  - "--skip-multus-binary-copy=false"
                resources:
                  requests:
                    cpu: "100m"
                    memory: "50Mi"
                  limits:
                    cpu: "100m"
                    memory: "50Mi"
                securityContext:
                  privileged: true
                  capabilities:
                    add: ["SYS_ADMIN"]
                volumeMounts:
                  - name: cni
                    mountPath: /host/etc/cni/net.d
                  - name: cnibin
                    mountPath: /host/opt/cni/bin
                  # Mount kubeconfig from outside CNI dir to prevent thin_entrypoint overwrite
                  - name: multus-kubeconfig
                    mountPath: /var/lib/multus
                    readOnly: true
                # No KUBERNETES_SERVICE_HOST/PORT env vars needed - we use static config
            volumes:
              - name: cni
                hostPath:
                  path: "{{ cni_conf_dir_edge }}"
              - name: cnibin
                hostPath:
                  path: "{{ cni_bin_dir_edge }}"
              - name: multus-kubeconfig
                hostPath:
                  path: /var/lib/multus
                  type: Directory
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Wait for both Multus DaemonSets
- name: Wait for Multus worker DS
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: DaemonSet
    name: multus-worker
    namespace: kube-system
    kubeconfig: "/home/vagrant/kubeconfig"
  register: multus_worker_ds
  until: (multus_worker_ds.resources|length)>0 and (multus_worker_ds.resources.0.status.numberReady|default(0)|int) >= 1
  retries: 24
  delay: 5
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# k3s looks for CNI plugins in /var/lib/rancher/k3s/data/cni/ (without bin/)
# Multus DaemonSet copies binary to /var/lib/rancher/k3s/data/cni/bin/multus
# Create symlink so k3s can find it (same pattern as ovs, whereabouts, static)
- name: Create k3s symlink for Multus binary (worker)
  file:
    src: "/var/lib/rancher/k3s/data/cni/bin/multus"
    dest: "/var/lib/rancher/k3s/data/cni/multus"
    state: link
    force: yes
  delegate_to: "{{ groups['workers'][0] }}"
  run_once: true

- name: Wait for Multus edge DS
  kubernetes.core.k8s_info:
    api_version: apps/v1
    kind: DaemonSet
    name: multus-edge
    namespace: kube-system
    kubeconfig: "/home/vagrant/kubeconfig"
  register: multus_edge_ds
  until: (multus_edge_ds.resources|length)>0 and (multus_edge_ds.resources.0.status.numberReady|default(0)|int) >= 1
  retries: 24
  delay: 5
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# Verify the token works
- name: Verify Multus token can list Pods in ns 5g
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Pod
    namespace: 5g
    kubeconfig: "/home/vagrant/kubeconfig"
  register: multus_list_check
  failed_when: multus_list_check.failed | default(false)
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

# ==============================================================
# CREATE NETWORK ATTACHMENT DEFINITIONS (NADs)
# These are used by pods with k8s.v1.cni.cncf.io/networks annotations
# ==============================================================

- name: Create NADs
  kubernetes.core.k8s:
    state: present
    definition: >-
      {{
        {
          "apiVersion": "k8s.cni.cncf.io/v1",
          "kind": "NetworkAttachmentDefinition",
          "metadata": {"name": item.name, "namespace": item.ns},
          "spec": {
            "config": (
              ({
                "cniVersion": "0.3.1",
                "type": (use_ovs_cni | ternary("ovs","bridge")),
                (use_ovs_cni | ternary("bridge","bridge")): item.bridge,
                "mtu": overlay_mtu
              } | combine(
                ({"ipam": {"type": "whereabouts","range": item.cidr,"rangeStart": item.start,"rangeEnd": item.end,"gateway": item.gw,"kubernetes": {"kubeconfig": cni_conf_dir_worker + "/" + whereabouts_kubeconfig_relpath}}}
                 if item.ns == "5g"
                 else {"ipam": {"type": "whereabouts","range": item.cidr,"rangeStart": item.start,"rangeEnd": item.end,"gateway": item.gw,"kubernetes": {"kubeconfig": cni_conf_dir_edge + "/" + whereabouts_kubeconfig_relpath}}})
                if (item.ipam | default("whereabouts")) == "whereabouts"
                else ({"ipam": {"type": "static"}}
                      if (item.ipam | default("")) == "static"
                      else {})
              ))
              | to_json
            )
          }
        }
      }}
    kubeconfig: "/home/vagrant/kubeconfig"
  loop: "{{ overlay_networks }}"
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true
