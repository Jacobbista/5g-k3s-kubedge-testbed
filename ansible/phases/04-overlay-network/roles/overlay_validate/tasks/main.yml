---
# Overlay validation: ensure Multus/OVS overlay is healthy and Flannel remains primary

- name: Validate DaemonSets status (multus, ovs setup, compat, binaries, cleanup)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    kubectl -n kube-system get ds kube-multus-ds ds-net-setup cni-bin-compat cni-binaries-install cni-conf-cleanup -o wide || true
  register: ds_overview
  changed_when: false
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Show DaemonSets overview
  debug:
    msg: "{{ ds_overview.stdout_lines }}"

- name: Assert 00-multus.conf is absent on all nodes
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    for p in $(kubectl -n kube-system get pods -l app=cni-conf-cleanup -o jsonpath='{.items[*].metadata.name}'); do
      kubectl -n kube-system exec "$p" -- sh -c 'test ! -f /host-cni-conf/00-multus.conf'
    done
    echo OK
  register: guard_multus_primary
  changed_when: false
  failed_when: guard_multus_primary.rc != 0
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Verify Flannel config exists in k3s CNI conf dir
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    # Check on master by inspecting worker node filesystem via cleanup DS pod
    for p in $(kubectl -n kube-system get pods -l app=cni-conf-cleanup -o jsonpath='{.items[*].metadata.name}'); do
      kubectl -n kube-system exec "$p" -- sh -c 'ls -1 /host-cni-conf | grep -E "flannel|10-.*flannel.*conflist"' || exit 1
    done
    echo OK
  register: flannel_primary
  changed_when: false
  failed_when: flannel_primary.rc != 0
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Verify Multus DS uses k3s paths
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    kubectl -n kube-system get ds kube-multus-ds -o yaml | egrep -A2 'CNI_(CONF|BIN)_DIR|hostPath'
  register: multus_paths
  changed_when: false
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Show Multus paths
  debug:
    msg: "{{ multus_paths.stdout_lines }}"

- name: Ensure netshoot test pods exist (created by multus role)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    kubectl -n 5g get pod n3test-worker n3test-edge
  register: netshoot_exist
  changed_when: false
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true
  failed_when: netshoot_exist.rc != 0

- name: Wait for netshoot pods to be Ready (60s)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    kubectl -n 5g wait --for=condition=Ready pod -l app=n3test --timeout=60s
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true
  changed_when: false

- name: Retrieve n3 IPs (worker and edge)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    # First try via Multus network-status annotation (most reliable)
    NAMESPACE="{{ nad_n3_namespace }}"
    NAD_NAME="{{ nad_n3_name }}"
    IF_EXP="{{ n3_interface_expected }}"
    IF_RE="{{ n3_interface_regex }}"
    annW=$(kubectl -n 5g get pod n3test-worker -o json | jq -r '.metadata.annotations["k8s.v1.cni.cncf.io/network-status"]')
    annE=$(kubectl -n 5g get pod n3test-edge   -o json | jq -r '.metadata.annotations["k8s.v1.cni.cncf.io/network-status"]')
    W_IP=$(echo "$annW" | jq -r 'try (fromjson | .[] | select((.name=="'"$NAMESPACE/$NAD_NAME"'") or (.interface=="'"$IF_EXP"'")) | .ips[0]) // empty')
    E_IP=$(echo "$annE" | jq -r 'try (fromjson | .[] | select((.name=="'"$NAMESPACE/$NAD_NAME"'") or (.interface=="'"$IF_EXP"'")) | .ips[0]) // empty')
    # Fallback to inside-pod ip discovery if annotation missing
    if [ -z "$W_IP" ]; then
      # Prefer expected iface; else any iface matching regex
      W_IP=$(kubectl -n 5g exec n3test-worker -c nsh -- sh -c "ip -o -4 addr show dev $IF_EXP 2>/dev/null | awk '{print \\$4}' | cut -d/ -f1 | head -n1 || true")
      if [ -z "$W_IP" ]; then
        IFSEL=$(kubectl -n 5g exec n3test-worker -c nsh -- sh -c "ip -o link show | awk -F: '{print \\$2}' | sed 's/ //g' | grep -E '$IF_RE' | head -n1" || true)
        [ -n "$IFSEL" ] && W_IP=$(kubectl -n 5g exec n3test-worker -c nsh -- sh -c "ip -o -4 addr show dev $IFSEL 2>/dev/null | awk '{print \\$4}' | cut -d/ -f1 | head -n1 || true")
      fi
    fi
    if [ -z "$E_IP" ]; then
      E_IP=$(kubectl -n 5g exec n3test-edge   -c nsh -- sh -c "ip -o -4 addr show dev $IF_EXP 2>/dev/null | awk '{print \\$4}' | cut -d/ -f1 | head -n1 || true")
      if [ -z "$E_IP" ]; then
        IFSEL=$(kubectl -n 5g exec n3test-edge -c nsh -- sh -c "ip -o link show | awk -F: '{print \\$2}' | sed 's/ //g' | grep -E '$IF_RE' | head -n1" || true)
        [ -n "$IFSEL" ] && E_IP=$(kubectl -n 5g exec n3test-edge -c nsh -- sh -c "ip -o -4 addr show dev $IFSEL 2>/dev/null | awk '{print \\$4}' | cut -d/ -f1 | head -n1 || true")
      fi
    fi
    echo WORKER=$W_IP
    echo EDGE=$E_IP
  register: n3_ips
  changed_when: false
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true

- name: Show retrieved n3 IPs
  debug:
    msg: "{{ n3_ips.stdout_lines }}"

- name: Ping worker→edge (overlay n3)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    EDGE_IP=$(echo '{{ n3_ips.stdout }}' | awk -F= '/EDGE=/{print $2}')
    kubectl -n 5g exec n3test-worker -c nsh -- ping -c 2 -W 2 "$EDGE_IP"
  register: ping_w2e
  failed_when: ping_w2e.rc != 0
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true
  when: n3_ips.stdout is search('EDGE=[0-9]')

- name: Ping edge→worker (overlay n3)
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    WORKER_IP=$(echo '{{ n3_ips.stdout }}' | awk -F= '/WORKER=/{print $2}')
    kubectl -n 5g exec n3test-edge -c nsh -- ping -c 2 -W 2 "$WORKER_IP"
  register: ping_e2w
  failed_when: ping_e2w.rc != 0
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true
  when: n3_ips.stdout is search('WORKER=[0-9]')

- name: Show overlay ping results
  debug:
    msg:
      - "worker→edge: {{ ping_w2e.stdout | default('') }}"
      - "edge→worker: {{ ping_e2w.stdout | default('') }}"

- name: Cleanup netshoot test pods after validation
  shell: |
    export KUBECONFIG=/home/vagrant/kubeconfig
    kubectl -n 5g delete pod n3test-worker n3test-edge --ignore-not-found=true --wait=true
    kubectl -n 5g delete pod -l app=n2test --ignore-not-found=true --wait=true
    kubectl -n mec delete pod -l app=n6etest --ignore-not-found=true --wait=true
    kubectl -n 5g delete pod -l app=n6ctest --ignore-not-found=true --wait=true
  register: n3_cleanup
  changed_when: true
  delegate_to: "{{ groups['masters'][0] }}"
  run_once: true


